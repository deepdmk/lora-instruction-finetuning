{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# LoRA Instruction Fine-Tuning\n",
    "\n",
    "This notebook demonstrates parameter-efficient instruction fine-tuning using LoRA (Low-Rank Adaptation) on a causal language model.\n",
    "\n",
    "## Overview\n",
    "- **Model**: OPT-350M\n",
    "- **Technique**: LoRA for parameter-efficient fine-tuning with Supervised Fine-Tuning (SFT)\n",
    "- **Dataset**: CodeAlpaca-20k - instruction-response pairs for code generation\n",
    "\n",
    "## Requirements\n",
    "- Python 3.9+\n",
    "- PyTorch 2.x\n",
    "- CUDA-capable GPU (recommended for training)\n",
    "- ~8GB GPU memory for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%pip install --user -qq datasets==2.20.0 trl==0.9.6 transformers==4.42.3 peft==0.11.1 tqdm==4.66.4 numpy==1.26.4 pandas==2.2.2 matplotlib==3.9.1 seaborn==0.13.2 scikit-learn==1.5.1 sacrebleu==2.4.2 --upgrade evaluate\n",
    "%pip install torch==2.3.0+cpu \\\n",
    "    --index-url https://download.pytorch.org/whl/cpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import matplotlib.pyplot as plt \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "device",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-prep",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "We use the CodeAlpaca-20k dataset, which contains programming instruction-response pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WzOT_CwDALWedTtXjwH7bA/CodeAlpaca-20k.json\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"CodeAlpaca-20k.json\", split=\"train\")\n",
    "dataset\n",
    "\n",
    "dataset[1000]\n",
    "dataset = dataset.filter(lambda example: example[\"input\"] == '')\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset_split['train']\n",
    "test_dataset = dataset_split['test']\n",
    "dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a small set of data for the resource limitation\n",
    "# This dataset will be only used for evaluation parts, not for the training\n",
    "tiny_test_dataset=test_dataset.select(range(10))\n",
    "tiny_train_dataset=train_dataset.select(range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-setup",
   "metadata": {},
   "source": [
    "## Model and Tokenizer Setup\n",
    "\n",
    "Load the base OPT-350M model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\", padding_side='left')\n",
    "# Set the pad token to the eos token in order to prevent potential issues with OPT models\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formatting",
   "metadata": {},
   "source": [
    "## Data Formatting Functions\n",
    "\n",
    "Format the dataset for instruction fine-tuning with clear instruction/response sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "format-funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Lora configuration\n",
    "def formatting_prompts_func(mydataset):\n",
    "    output_texts = []\n",
    "    for i in range(len(mydataset['instruction'])):\n",
    "        text = (\n",
    "            f\"### Instruction:\\n{mydataset['instruction'][i]}\"\n",
    "            f\"\\n\\n### Response:\\n{mydataset['output'][i]}</s>\"\n",
    "        )\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "# Define no response function\n",
    "def formatting_prompts_func_no_response(mydataset):\n",
    "    output_texts = []\n",
    "    for i in range(len(mydataset['instruction'])):\n",
    "        text = (\n",
    "            f\"### Instruction:\\n{mydataset['instruction'][i]}\"\n",
    "            f\"\\n\\n### Response:\\n\"\n",
    "        )\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prep-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_outputs = []\n",
    "instructions_with_responses = formatting_prompts_func(test_dataset)\n",
    "instructions = formatting_prompts_func_no_response(test_dataset)\n",
    "for i in tqdm(range(len(instructions_with_responses))):\n",
    "    tokenized_instruction_with_response = tokenizer(instructions_with_responses[i], return_tensors=\"pt\", max_length=1024, truncation=True, padding=False)\n",
    "    tokenized_instruction = tokenizer(instructions[i], return_tensors=\"pt\")\n",
    "    expected_output = tokenizer.decode(tokenized_instruction_with_response['input_ids'][0][len(tokenized_instruction['input_ids'][0])-1:], skip_special_tokens=True)\n",
    "    expected_outputs.append(expected_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('############## instructions ##############\\n' + instructions[0])\n",
    "print('############## instructions_with_responses ##############\\n' + instructions_with_responses[0])\n",
    "print('\\n############## expected_outputs ##############' + expected_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListDataset(Dataset):\n",
    "    def __init__(self, original_list):\n",
    "        self.original_list = original_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.original_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.original_list[i]\n",
    "\n",
    "instructions_torch = ListDataset(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_torch[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline",
   "metadata": {},
   "source": [
    "## Baseline Model Evaluation\n",
    "\n",
    "Generate outputs from the base OPT-350M model to establish a performance baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-pipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_pipeline = pipeline(\"text-generation\",\n",
    "                        model=model,\n",
    "                        tokenizer=tokenizer,\n",
    "                        device=device,\n",
    "                        batch_size=2,\n",
    "                        max_length=50,\n",
    "                        truncation=True,\n",
    "                        padding=False,\n",
    "                        return_full_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-gen",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Due to resource limitation, only apply the function on 3 records using \"instructions_torch[:10]\"\n",
    "    pipeline_iterator= gen_pipeline(instructions_torch[:3], \n",
    "                                    max_length=50, # this is set to 50 due to resource constraint, using a GPU, you can increase it to the length of your choice\n",
    "                                    num_beams=5,\n",
    "                                    early_stopping=True,)\n",
    "\n",
    "generated_outputs_base = []\n",
    "for text in pipeline_iterator:\n",
    "    generated_outputs_base.append(text[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-show",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print('@@@@@@@@@@@@@@@@@@@@')\n",
    "    print('@@@@@ Instruction '+ str(i+1) +': ')\n",
    "    print(instructions[i])\n",
    "    print('\\n\\n')\n",
    "    print('@@@@@ Expected response '+ str(i+1) +': ')\n",
    "    print(expected_outputs[i])\n",
    "    print('\\n\\n')\n",
    "    print('@@@@@ Generated response '+ str(i+1) +': ')\n",
    "    print(generated_outputs_base[i])\n",
    "    print('\\n\\n')\n",
    "    print('@@@@@@@@@@@@@@@@@@@@')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "results_base = sacrebleu.compute(predictions=generated_outputs_base,\n",
    "                                 references=expected_outputs)\n",
    "\n",
    "print(list(results_base.keys()))\n",
    "print(round(results_base[\"score\"], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lora-config",
   "metadata": {},
   "source": [
    "## LoRA Configuration\n",
    "\n",
    "Configure Low-Rank Adaptation for parameter-efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configure-lora",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,  # Low-rank dimension\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Modules to apply LoRA\n",
    "    lora_dropout=0.1,  # Dropout rate\n",
    "    task_type=TaskType.CAUSAL_LM  # Task type should be causal language model\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-config",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Set up training arguments and data collator. Loss is only calculated on the response portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collator",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"### Response:\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=\"/tmp\",\n",
    "    num_train_epochs=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=2,  # Reduce batch size\n",
    "    per_device_eval_batch_size=2,  # Reduce batch size\n",
    "    max_seq_length=1024,\n",
    "    do_eval=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-exec",
   "metadata": {},
   "source": [
    "### Training Execution\n",
    "\n",
    "Run the training loop to fine-tune the LoRA adapters for 10 epochs.\n",
    "\n",
    "**Note**: Training takes approximately 30-60 minutes on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "log",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_history_lora = trainer.state.log_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz",
   "metadata": {},
   "source": [
    "### Training Loss Visualization\n",
    "\n",
    "Plot the training loss curve to verify the model is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = [log[\"loss\"] for log in log_history_lora if \"loss\" in log]\n",
    "\n",
    "# Plot the training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./instruction_tuning_final_model_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lora-eval",
   "metadata": {},
   "source": [
    "## LoRA Fine-Tuned Model Evaluation\n",
    "\n",
    "Generate outputs from the LoRA fine-tuned model and compare against baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lora-pipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_pipeline = pipeline(\"text-generation\", \n",
    "                        model=model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        device=device, \n",
    "                        batch_size=2, \n",
    "                        max_length=50, \n",
    "                        truncation=True, \n",
    "                        padding=False,\n",
    "                        return_full_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lora-gen",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Due to resource limitation, only apply the function on 3 records using \"instructions_torch[:10]\"\n",
    "    pipeline_iterator= gen_pipeline(instructions_torch[:3],\n",
    "                                max_length=50, # this is set to 50 due to resource constraint, using a GPU, you can increase it to the length of your choice\n",
    "                                num_beams=5,\n",
    "                                early_stopping=True,)\n",
    "generated_outputs_lora = []\n",
    "for text in pipeline_iterator:\n",
    "    generated_outputs_lora.append(text[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lora-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_outputs_lora[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results",
   "metadata": {},
   "source": [
    "### Results Comparison\n",
    "\n",
    "Compare outputs and evaluate using SACREBLEU metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print('@@@@@@@@@@@@@@@@@@@@')\n",
    "    print('@@@@@ Instruction '+ str(i+1) +': ')\n",
    "    print(instructions[i])\n",
    "    print('\\n\\n')\n",
    "    print('@@@@@ Expected response '+ str(i+1) +': ')\n",
    "    print(expected_outputs[i])\n",
    "    print('\\n\\n')\n",
    "    print('@@@@@ Generated response '+ str(i+1) +': ')\n",
    "    print(generated_outputs_lora[i])\n",
    "    print('\\n\\n')\n",
    "    print('@@@@@@@@@@@@@@@@@@@@')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "results_lora = sacrebleu.compute(predictions=generated_outputs_lora,\n",
    "                                 references=expected_outputs)\n",
    "print(list(results_lora.keys()))\n",
    "print(round(results_lora[\"score\"], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alt",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Alternative Implementation\n",
    "\n",
    "Alternative formatting and model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alt-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the formatting function for the response template\n",
    "def formatting_prompts_response_template(mydataset):\n",
    "    output_texts = []\n",
    "    for i in range(len(mydataset['instruction'])):\n",
    "        text = (\n",
    "            f\"### Question:\\n{mydataset['instruction'][i]}\"\n",
    "            f\"\\n\\n### Answer:\\n{mydataset['output'][i]}</s>\"\n",
    "        )\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alt-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the formatting function for the response template for no response\n",
    "def formatting_prompts_response_template_no_response(mydataset):\n",
    "    output_texts = []\n",
    "    for i in range(len(mydataset['instruction'])):\n",
    "        text = (\n",
    "            f\"### Question:\\n{mydataset['instruction'][i]}\"\n",
    "            f\"\\n\\n### Answer:\\n\"\n",
    "        )\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alt-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model_name = \"EleutherAI/gpt-neo-125m\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alt-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Low-rank dimension\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Modules to apply LoRA\n",
    "    lora_dropout=0.1,  # Dropout rate\n",
    "    task_type=TaskType.CAUSAL_LM  # Task type should be causal language model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alt-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
